import tensorflow as tf
import numpy as np
import os
import seaborn as sn
import matplotlib.pyplot as plt
import cv2
import pandas as pd
from tqdm import tqdm
from sklearn import decomposition
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.utils import shuffle
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix


def load_data():
    """
        Load the data:
            - 14,034 images to train the network.
            - 3,000 images to evaluate how accurately the network learned to classify images.
    """
    datasets = ['DNAFNLCHNG/DNAINKtrain',
                'DNAFNLCHNG/DNAINKtest']
    output = []
    # Iterate through training and test sets
    for dataset in datasets:
        images = []
        labels = []
        print("Loading {}".format(dataset))
        # Iterate through each folder corresponding to a category
        for folder in os.listdir(dataset):
            label = class_names_label[folder]
            # Iterate through each image in our folder
            for file in tqdm(os.listdir(os.path.join(dataset, folder))):
                # Get the path name of the image
                img_path = os.path.join(os.path.join(dataset, folder), file)
                # Open and resize the img
                image = cv2.imread(img_path)
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                image = cv2.resize(image, IMAGE_SIZE)
                # Append the image and its corresponding label to the output
                images.append(image)
                labels.append(label)
        images = np.array(images, dtype='float32')
        labels = np.array(labels, dtype='int32')
        output.append((images, labels))
    return output


def display_random_image(class_names, images, labels):
    """
        Display a random image from the images array and its correspond label from the labels array.
    """
    index = np.random.randint(images.shape[0])
    plt.figure()
    plt.imshow(images[index])
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.title('Image #{} : '.format(index) + class_names[labels[index]])
    plt.show()


def display_examples(class_names, images, labels):
    """
        Display 25 images from the images array with its corresponding labels
    """
    fig = plt.figure(figsize=(10, 10))
    fig.suptitle("Some examples of images of the dataset", fontsize=16)
    for i in range(25):
        plt.subplot(5, 5, i + 1)
        plt.xticks([])
        plt.yticks([])
        plt.grid(False)
        plt.imshow(images[i], cmap=plt.cm.binary)
        plt.xlabel(class_names[labels[i]])
    plt.show()


def imageAugmentor():
    data_generator = ImageDataGenerator(rotation_range=180)
    plot(data_generator)
    data_generator = ImageDataGenerator(featurewise_center=False,
                                        width_shift_range=0.65)
    plot(data_generator)
    data_generator = ImageDataGenerator(featurewise_center=False,
                                        width_shift_range=0.65)
    plot(data_generator)
    data_generator = ImageDataGenerator(vertical_flip=True,
                                        zoom_range=[0.2, 0.9],
                                        width_shift_range=0.2)
    plot(data_generator)
    data_generator = ImageDataGenerator(horizontal_flip=True,
                                        zoom_range=[1, 1.5],
                                        width_shift_range=0.2)
    plot(data_generator)
    data_generator = ImageDataGenerator(width_shift_range=[0.1, 0.5])
    plot(data_generator)
    data_generator = ImageDataGenerator(zoom_range=[1, 2], rotation_range=260)
    plot(data_generator)


def plot(data_generator, images=None):
    """
    Plots 4 images generated by an object of the ImageDataGenerator class.
    """
    data_generator.fit(images)
    image_iterator = data_generator.flow(images)
    # Plot the images given by the iterator
    fig, rows = plt.subplots(nrows=1, ncols=4, figsize=(18, 18))
    for row in rows:
        row.imshow(image_iterator.next()[0].astype('int'))
        row.axis('on')
    plt.show()


def plot_accuracy_loss(history):
    """
        Plot the accuracy and the loss during the training of the nn.
    """
    # fig = plt.figure(figsize=(10, 5))
    # Plot accuracy
    plt.subplot(221)
    plt.plot(history.history['accuracy'], 'bo--', label="accuracy")
    plt.plot(history.history['val_accuracy'], 'ro--', label="val_accuracy")
    plt.title("train_acc vs val_acc")
    plt.ylabel("accuracy")
    plt.xlabel("epochs")
    plt.legend()
    # Plot loss function
    plt.subplot(222)
    plt.plot(history.history['loss'], 'bo--', label="loss")
    plt.plot(history.history['val_loss'], 'ro--', label="val_loss")
    plt.title("train_loss vs val_loss")
    plt.ylabel("loss")
    plt.xlabel("epochs")
    plt.legend()
    plt.show()


sn.set(font_scale=1.4)
class_names = ['0 mM', '10 mM', '50 mM', '200 mM', '500 mM']
class_names_label = {class_name: i for i, class_name in enumerate(class_names)}
nb_classes = len(class_names)
IMAGE_SIZE = (200, 200)
(train_images, train_labels), (test_images, test_labels) = load_data()
train_images, train_labels = shuffle(train_images, train_labels, random_state=25)
n_train = train_labels.shape[0]
n_test = test_labels.shape[0]
print("Number of training examples: {}".format(n_train))
print("Number of testing examples: {}".format(n_test))
print("Each image is of size: {}".format(IMAGE_SIZE))
_, train_counts = np.unique(train_labels, return_counts=True)
_, test_counts = np.unique(test_labels, return_counts=True)
pd.DataFrame({'train': train_counts,
              'test': test_counts},
             index=class_names
             ).plot.bar()
plt.show()
plt.pie(train_counts,
        labels=class_names,
        autopct='%.2f')
plt.axis('equal')
plt.title('Proportion of each observed category')
plt.show()
train_images = train_images / 255.0
test_images = test_images / 255.0
display_random_image(class_names, train_images, train_labels)
display_examples(class_names, train_images, train_labels)
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(200, 200, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation=tf.nn.relu),
    tf.keras.layers.Dense(6, activation=tf.nn.softmax)
])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history = model.fit(train_images, train_labels, batch_size=128, epochs=30, validation_split=0.2)
plot_accuracy_loss(history)
test_loss = model.evaluate(test_images, test_labels)
predictions = model.predict(test_images)  # Vector of probabilities
pred_labels = np.argmax(predictions, axis=1)  # We take the highest probability
display_random_image(class_names, test_images, pred_labels)
CM = confusion_matrix(test_labels, pred_labels)
ax = plt.axes()
sn.heatmap(CM, annot=True,
           annot_kws={"size": 10},
           xticklabels=class_names,
           yticklabels=class_names, ax=ax)
ax.set_title('Confusion matrix')
plt.show()
# from keras.models import Model
from keras.applications.vgg16 import VGG16
# from keras.preprocessing import image
from keras.applications.vgg16 import preprocess_input
model = VGG16(weights='../input/vgg16-weights/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top=False)
train_features = model.predict(train_images)
test_features = model.predict(test_images)
n_train = train_features.shape
n_test, x, y, z = test_features.shape
numFeatures = x * y * z
pca = decomposition.PCA(n_components=2)
X = train_features.reshape((n_train, x * y * z))
pca.fit(X)
C = pca.transform(X)  # Representation des individus dans les nouveaux axe
C1 = C[:, 0]
C2 = C[:, 1]
# Figures
plt.subplots(figsize=(10, 10))
for i, class_name in enumerate(class_names):
    plt.scatter(C1[train_labels == i][:1000], C2[train_labels == i][:1000], label=class_name, alpha=0.4)
plt.legend()
plt.title("PCA Projection")
plt.show()
model2 = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(x, y, z)),
    tf.keras.layers.Dense(50, activation=tf.nn.relu),
    tf.keras.layers.Dense(6, activation=tf.nn.softmax)
])
model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history2 = model2.fit(train_features, train_labels, batch_size=128, epochs=30, validation_split=0.2)
plot_accuracy_loss(history)
test_loss = model2.evaluate(test_features, test_labels)
np.random.seed(seed=1997)
# Number of estimators
n_estimators = 10
# Proportion of samples to use to train each training
max_samples = 0.8
max_samples *= n_train
max_samples = int(max_samples)
models = list()
random = np.random.randint(50, 100, size=n_estimators)
for i in range(n_estimators):
    # Model
    model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape=(x, y, z)),
                                 # One layer with random size
                                 tf.keras.layers.Dense(random[i], activation=tf.nn.relu),
                                 tf.keras.layers.Dense(6, activation=tf.nn.softmax)
                                 ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    # Store model
    models.append(model)
histories = []
for i in range(n_estimators):
    # Train each model on a bag of the training data
    train_idx = np.random.choice(len(train_features), size=max_samples)
    histories.append(models[i].fit(train_features[train_idx], train_labels[train_idx],
                                   batch_size=128, epochs=30, validation_split=0.1))
predictions = []
for i in range(n_estimators):
    predictions.append(models[i].predict(test_features))
predictions = np.array(predictions)
predictions = predictions.sum(axis=0)
pred_labels = predictions.argmax(axis=1)
from sklearn.metrics import accuracy_score
print("Accuracy : {}".format(accuracy_score(test_labels, pred_labels)))
CM = confusion_matrix(test_labels, pred_labels)
ax = plt.axes()
sn.heatmap(CM, annot=True,
           annot_kws={"size": 10},
           xticklabels=class_names,
           yticklabels=class_names, ax=ax)
ax.set_title('Confusion matrix')
plt.show()
from keras.applications.vgg16 import VGG16
from keras.preprocessing import image
from keras.applications.vgg16 import preprocess_input
from keras.models import Model
model = VGG16(weights='../input/vgg16-weights/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top=False)
model = Model(inputs=model.inputs, outputs=model.layers[-5].output)
train_features = model.predict(train_images)
test_features = model.predict(test_images)
from keras.layers import Input, Dense, Conv2D, Activation, MaxPooling2D, Flatten
model2 = VGG16(weights='../input/vgg16-weights/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top=False)
input_shape = model2.layers[-4].get_input_shape_at(0)  # get the input shape of desired layer
layer_input = Input(shape=(12, 12, 512))  # a new input tensor to be able to feed the desired layer
# https://stackoverflow.com/questions/52800025/keras-give-input-to-intermediate-layer-and-get-final-output
x = layer_input
for layer in model2.layers[-4::1]:
    x = layer(x)
x = Conv2D(64, (3, 3), activation='relu')(x)
x = MaxPooling2D(pool_size=(2, 2))(x)
x = Flatten()(x)
x = Dense(100, activation='relu')(x)
x = Dense(6, activation='softmax')(x)
# create the model
new_model = Model(layer_input, x)
new_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
new_model.summary()
history = new_model.fit(train_features, train_labels, batch_size=128, epochs=15, validation_split=0.2)
plot_accuracy_loss(history)
from sklearn.metrics import accuracy_score
predictions = new_model.predict(test_features)
pred_labels = np.argmax(predictions, axis=1)
print("Accuracy : {}".format(accuracy_score(test_labels, pred_labels)))
CM = confusion_matrix(test_labels, pred_labels)
ax = plt.axes()
sn.heatmap(CM, annot=True,
           annot_kws={"size": 10},
           xticklabels=class_names,
           yticklabels=class_names, ax=ax)
ax.set_title('Confusion matrix')
plt.show()
